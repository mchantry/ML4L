{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e3cfeb",
   "metadata": {},
   "source": [
    "# Learning MODIS LST from ERA features\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "\n",
    "* `X data`. This is the ERA `.grib` files.\n",
    "\n",
    "* `Y data`. This is the MODIS `.tif` files.\n",
    "\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "\n",
    "1. Convert The ERA `.grib` files \n",
    "\n",
    "   These are are on a monthly grain and quite large. It is useful to convert these into `NetCDF` files on a hourly grain. This makes IO and operations on these files much faster. This is done in`convert_grib_to_netcdf.py`. See also `01.Convert_grib_to_NetCDF.ipynb`. In this step we also convert the longtiude to `long1` format so as to match the format in the MODIS data.\n",
    "\n",
    "2. Join the ERA and MODIS data\n",
    "\n",
    "   These are joined in both time/space to produce a collection of hourly `.pkl` files. This is done in `join_MODIS_with_ERA.py`. See also `02.Join_MODIS_with_ERA_NetCDF.ipynb` and `03.Join_MODIS_with_ERA_faiss.ipynb`\n",
    "   \n",
    "3. Create a single ML dataframe\n",
    "\n",
    "    We then join all these hourly files into a single `ML_data.pkl` which can be easily loaded. \n",
    "   \n",
    "   \n",
    "   \n",
    "### Processing + Viz\n",
    "\n",
    "* Given our cleaned data we can train our model. We use a simple tapered sequential NN. We are currently taking 2018 as a training set and 2020 as a testing set, holding 2019 as a validation set. See `04.Train-NN-lite.py` and `scripts/train_and_predict_lite.py`. This script creates a directory which holds the model, training history and predictions.\n",
    "\n",
    "* Predictions can be visualized and compared in `05.Plot_Model.ipynb` \n",
    "\n",
    "\n",
    "\n",
    "### Further information \n",
    "We have 4 “types” of ERA file, corresponding to different model fields.\n",
    "\n",
    "    They are:\n",
    "\n",
    "    1. `sfc_skin_unstructured_...`. 10 years of data (2010-2020), hourly grain\n",
    "\n",
    "        aluvp, aluvd, alnip, alnid, cl, cvl, mvh, istl1, istl2, slt, sdfor, z, sd, store, isor, anor, spor, 2d, lsm, fal\n",
    "        \n",
    "    2. `sfc_unstructured_...` 3 years of data (2018,209,2020), hourly grain\n",
    "\n",
    "        sp, msl, 10u, 10v, 2t\n",
    "    \n",
    "    3. `sfc_skin2_unstructured_...` 10 years of data, only at 06:00 and 18:00\n",
    "\n",
    "        ssrd, strd, tp\n",
    "\n",
    "    4. `ml_skin_unstructured...` 10 years of data.\n",
    "\n",
    "        some spherical harmonic fields\n",
    "  \n",
    "    \n",
    "We will use exclusively `sfc_unstructured_` and `sfc_skin_unstructured_` for 2018-2020\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Questions and uncertainties\n",
    "\n",
    "* Is it better to do a \"single nearest match\" where each ERA data point is joined to the nearest MODIS point or else a \"group averaged match\" where every MODIS point is matched to the nearest ERA point, and we then take an average over that set, subject to some cutoff tolerance (e.g. only count MODIS data points if they are <50km of the ERA point). We seem to train better in the latter case.\n",
    "\n",
    "* The nearest neighbour matching employs the [faiss library](https://github.com/facebookresearch/faiss) for fast k-nearest neighbours on GPU. However this does not allow for custom metrics (e.g. Haversine) and instead used the L2 (squared) norm from the latitude/longitude coordinates. Naturally these are equivalent under small angle approximation, could be some overlooked edge case\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6744a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b643e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
